{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "class Model(object):\n",
    "    \n",
    "    def __init__(self,name,X,Y,modelObj,test=False,**modelParams):\n",
    "        self.params = modelParams\n",
    "        self.modelObj = modelObj.set_params(**self.params)  \n",
    "        self.name = name\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.initValues(test)   \n",
    "    \n",
    "    def initValues(self, test):\n",
    "        start = time.time()\n",
    "        self.modObjFit = self.modelObj.fit(self.X, self.Y)\n",
    "        end = time.time()\n",
    "        self.timeToRun = end - start\n",
    "        self.cvTrain = cross_val_score(self.modelObj, self.X, self.Y, cv=5)\n",
    "        self.cvTrainMean = np.mean(self.cvTrain)\n",
    "        self.cvTrainRange = max(self.cvTrain) - min(self.cvTrain)\n",
    "    \n",
    "    def performance(self, boxPlot=True):\n",
    "        print (\"Model:\\t\\t\" + str(self.name))\n",
    "        print (\"CV Mean:\\t\" + str(self.cvTrainMean))\n",
    "        print (\"CV Range:\\t\" + str(self.cvTrainRange))\n",
    "        print (\"Train Time:\\t\" + str(self.timeToRun))\n",
    "        print (\"CV Scores: \")\n",
    "        print (self.cvTrain)\n",
    "        if boxPlot:\n",
    "            fig = plt.figure()\n",
    "            title = \"Performance: %s\" % (self.name)\n",
    "            fig.suptitle(title)\n",
    "            ax = fig.add_subplot(111)\n",
    "            plt.boxplot(self.cvTrain, showmeans=True)\n",
    "            ax.set_xticklabels(self.name)\n",
    "            plt.show()      \n",
    "    \n",
    "    def compareBox(self,modelList,filterResult=0, newTitle=''):\n",
    "        results = []\n",
    "        names = []\n",
    "        \n",
    "        results.append(self.cvTrain)\n",
    "        names.append(self.name)\n",
    "        \n",
    "        for rightModel in modelList:\n",
    "            if filterResult > 0:\n",
    "                if rightModel.cvTrain.mean() > filterResult:\n",
    "                    results.append(rightModel.cvTrain)\n",
    "                    names.append(rightModel.name)\n",
    "            else:\n",
    "                results.append(rightModel.cvTrain)\n",
    "                names.append(rightModel.name)\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        if newTitle != '':\n",
    "            title = newTitle\n",
    "        else:\n",
    "            title = \"Performance: %s\" % (self.name)\n",
    "        fig.suptitle(title)\n",
    "        ax = fig.add_subplot(111)\n",
    "        \n",
    "        plt.boxplot(results, showmeans=True)\n",
    "        ax.set_xticklabels(names)\n",
    "        plt.setp(ax.get_xticklabels(), rotation=90)\n",
    "        plt.show()\n",
    "        \n",
    "    def fullCompareList(self,listOfModels,folds=False):\n",
    "        listOfModels.insert(0,self)\n",
    "        dfout = pd.DataFrame()\n",
    "        dataForFrame = []\n",
    "        cvNameList= []\n",
    "        for model in listOfModels:\n",
    "            dataRowForFrame = {}\n",
    "            dataRowForFrame = {'Name': model.name,\n",
    "                                'Time':model.timeToRun,\n",
    "                                'CV Mean':model.cvTrainMean,\n",
    "                                'CV Range': model.cvTrainRange}\n",
    "            if folds:  \n",
    "                buildCvNameList = False\n",
    "                if not cvNameList:\n",
    "                    buildCvNameList=True\n",
    "                for idx,cv in enumerate(model.cvTrain):\n",
    "                    cvName = 'CV Fold ' + str(idx+1)\n",
    "                    if buildCvNameList:\n",
    "                        cvNameList.append(cvName)\n",
    "                    dataRowForFrame[cvName] = cv\n",
    "            dataForFrame.append(dataRowForFrame)\n",
    "        colOrder = ['Name', 'CV Mean', 'CV Range', 'Time']\n",
    "        colOrder += cvNameList  \n",
    "        dfOut = pd.DataFrame(dataForFrame)\n",
    "        dfOut = dfOut[colOrder]\n",
    "        display(dfOut)\n",
    "        lgrbg = listOfModels.pop(0)\n",
    "        self.compareBox(listOfModels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Launch the installer to download \"gutenberg\" and \"stop words\" corpora.\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "# Import the data we just downloaded and installed.\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "\n",
    "# Grab and process the raw data.\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepGutenTextRaw (title):\n",
    "    \n",
    "    book = gutenberg.raw(title)\n",
    "    \n",
    "    # Remove Title\n",
    "    pattern = \"[\\[].*?[\\]]\"\n",
    "    book_noTitle = re.sub(pattern, \"\", book[0:9999])\n",
    "    \n",
    "    # recognize: the double dash '--'\n",
    "    book_noTitle = re.sub(r'--',' ',book_noTitle)\n",
    "    book_noTitle = re.sub(\"[\\[].*?[\\]]\", \"\", book_noTitle)\n",
    "    book_noTitle = ' '.join(book_noTitle.split())\n",
    "    \n",
    "    # Now we'll match and remove chapter headings and make lower\n",
    "    book_noChaps = re.sub(r'chapter *.', '', book_noTitle.lower())\n",
    "\n",
    "    nlp = spacy.load('en')\n",
    "    book_doc = nlp(book_noChaps)\n",
    "    \n",
    "    # Let's explore the objects we've built.\n",
    "    print(\"The {} object is a {} object.\".format(title, type(book_doc)))\n",
    "    print(\"It is {} tokens long\".format(len(book_doc)))\n",
    "    print(\"The first three tokens are '{}'\".format(book_doc[:3]))\n",
    "    print(\"The type of each token is {}\".format(type(book_doc[0])))\n",
    "    \n",
    "    \n",
    "    #from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    #lmtzr = WordNetLemmatizer()\n",
    "    #book_doc = [ lmtzr.lemmatize(token.text) for token in book_doc]\n",
    "    #remove stop words\n",
    "    \n",
    "    book_sents = [[sent, title.split('-', 2)[0]] for sent in book_doc.sents]\n",
    "\n",
    "    return book_doc,book_sents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The melville-moby_dick.txt object is a <class 'spacy.tokens.doc.Doc'> object.\n",
      "It is 2095 tokens long\n",
      "The first three tokens are 'etymology. ('\n",
      "The type of each token is <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "moby_doc,moby_sents = prepGutenTextRaw('melville-moby_dick.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whitman-leaves.txt object is a <class 'spacy.tokens.doc.Doc'> object.\n",
      "It is 2124 tokens long\n",
      "The first three tokens are 'come, said'\n",
      "The type of each token is <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "leaves_doc, leaves_sents = prepGutenTextRaw('whitman-leaves.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(etymology, .)</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((, supplied, by, a, late, consumptive, usher,...</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(he, was, ever, dusting, his, old, lexicons, a...</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(he, loved, to, dust, his, old, grammars, ;, i...</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(while, you, take, in, hand, to, school, other...</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0         1\n",
       "0                                     (etymology, .)  melville\n",
       "1  ((, supplied, by, a, late, consumptive, usher,...  melville\n",
       "2  (he, was, ever, dusting, his, old, lexicons, a...  melville\n",
       "3  (he, loved, to, dust, his, old, grammars, ;, i...  melville\n",
       "4  (while, you, take, in, hand, to, school, other...  melville"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = pd.DataFrame(moby_sents + leaves_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df\n",
    "\n",
    "# Set up the bags.\n",
    "mobywords = bag_of_words(moby_doc)\n",
    "leaveswords = bag_of_words(leaves_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(mobywords + leaveswords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pliny</th>\n",
       "      <th>outward</th>\n",
       "      <th>athwart</th>\n",
       "      <th>want</th>\n",
       "      <th>small</th>\n",
       "      <th>go</th>\n",
       "      <th>unfix'd</th>\n",
       "      <th>chemist</th>\n",
       "      <th>hwal</th>\n",
       "      <th>parmacetti</th>\n",
       "      <th>...</th>\n",
       "      <th>convivial</th>\n",
       "      <th>altogether</th>\n",
       "      <th>baleine</th>\n",
       "      <th>hackluyt</th>\n",
       "      <th>emotion</th>\n",
       "      <th>annal</th>\n",
       "      <th>play</th>\n",
       "      <th>artificial</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(etymology, .)</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>((, supplied, by, a, late, consumptive, usher,...</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(he, was, ever, dusting, his, old, lexicons, a...</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(he, loved, to, dust, his, old, grammars, ;, i...</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(while, you, take, in, hand, to, school, other...</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 999 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  pliny outward athwart want small go unfix'd chemist hwal parmacetti  \\\n",
       "0     0       0       0    0     0  0       0       0    0          0   \n",
       "1     0       0       0    0     0  0       0       0    0          0   \n",
       "2     0       0       0    0     0  0       0       0    0          0   \n",
       "3     0       0       0    0     0  0       0       0    0          0   \n",
       "4     0       0       0    0     0  0       0       0    0          0   \n",
       "\n",
       "      ...     convivial altogether baleine hackluyt emotion annal play  \\\n",
       "0     ...             0          0       0        0       0     0    0   \n",
       "1     ...             0          0       0        0       0     0    0   \n",
       "2     ...             0          0       0        0       0     0    0   \n",
       "3     ...             0          0       0        0       0     0    0   \n",
       "4     ...             0          0       0        1       0     0    0   \n",
       "\n",
       "  artificial                                      text_sentence text_source  \n",
       "0          0                                     (etymology, .)    melville  \n",
       "1          0  ((, supplied, by, a, late, consumptive, usher,...    melville  \n",
       "2          0  (he, was, ever, dusting, his, old, lexicons, a...    melville  \n",
       "3          0  (he, loved, to, dust, his, old, grammars, ;, i...    melville  \n",
       "4          0  (while, you, take, in, hand, to, school, other...    melville  \n",
       "\n",
       "[5 rows x 999 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features. This can take a while to run.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = word_counts.drop(['text_source', 'text_sentence'], 1)\n",
    "y = word_counts['text_source']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pliny</th>\n",
       "      <th>outward</th>\n",
       "      <th>athwart</th>\n",
       "      <th>want</th>\n",
       "      <th>small</th>\n",
       "      <th>go</th>\n",
       "      <th>unfix'd</th>\n",
       "      <th>chemist</th>\n",
       "      <th>hwal</th>\n",
       "      <th>parmacetti</th>\n",
       "      <th>...</th>\n",
       "      <th>begin</th>\n",
       "      <th>generally</th>\n",
       "      <th>convivial</th>\n",
       "      <th>altogether</th>\n",
       "      <th>baleine</th>\n",
       "      <th>hackluyt</th>\n",
       "      <th>emotion</th>\n",
       "      <th>annal</th>\n",
       "      <th>play</th>\n",
       "      <th>artificial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 997 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  pliny outward athwart want small go unfix'd chemist hwal parmacetti  \\\n",
       "0     0       0       0    0     0  0       0       0    0          0   \n",
       "1     0       0       0    0     0  0       0       0    0          0   \n",
       "2     0       0       0    0     0  0       0       0    0          0   \n",
       "3     0       0       0    0     0  0       0       0    0          0   \n",
       "4     0       0       0    0     0  0       0       0    0          0   \n",
       "\n",
       "     ...     begin generally convivial altogether baleine hackluyt emotion  \\\n",
       "0    ...         0         0         0          0       0        0       0   \n",
       "1    ...         0         0         0          0       0        0       0   \n",
       "2    ...         0         0         0          0       0        0       0   \n",
       "3    ...         0         0         0          0       0        0       0   \n",
       "4    ...         0         0         0          0       0        1       0   \n",
       "\n",
       "  annal play artificial  \n",
       "0     0    0          0  \n",
       "1     0    0          0  \n",
       "2     0    0          0  \n",
       "3     0    0          0  \n",
       "4     0    0          0  \n",
       "\n",
       "[5 rows x 997 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\t\tVanilla Reg\n",
      "CV Mean:\t0.73278014857\n",
      "CV Range:\t0.342105263158\n",
      "Train Time:\t0.010136127471923828\n",
      "CV Scores: \n",
      "[ 0.89473684  0.55263158  0.71052632  0.78378378  0.72222222]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEVCAYAAAAM3jVmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQEwgncEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a16d42e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vanilla \n",
    "vanRegModelParams = {'C': 1e9,\n",
    "               'penalty': 'l2'}\n",
    "vanRegModel = Model('Vanilla Reg',X,y,LogisticRegression(),**vanRegModelParams)\n",
    "vanRegModel.performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepGutenTextParas (title):\n",
    "    \n",
    "    book = gutenberg.paras(title)\n",
    "    book = book[0:9999]\n",
    "    \n",
    "    book_paras=[]\n",
    "    for paragraph in book:\n",
    "        para=paragraph[0]\n",
    "        #removing the double-dash from all words\n",
    "        para=[re.sub(r'--','',word) for word in para]\n",
    "        #Forming each paragraph into a string and adding it to the list of strings.\n",
    "        book_paras.append(' '.join(para))\n",
    "    \n",
    "    book_df = pd.DataFrame([[para, title.split('-', 2)[0]] for para in book_paras])\n",
    "    display(book_df.head())\n",
    "\n",
    "    return book_df\n",
    "\n",
    "\n",
    "    #book_sents = [[sent, title.split('-', 2)[0]] for sent in book_doc.sents]\n",
    "\n",
    "    #return book_doc,book_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ Moby Dick by Herman Melville 1851 ]</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ETYMOLOGY .</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>( Supplied by a Late Consumptive Usher to a Gr...</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The pale Usher  threadbare in coat , heart , b...</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\" While you take in hand to school others , an...</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0         1\n",
       "0              [ Moby Dick by Herman Melville 1851 ]  melville\n",
       "1                                        ETYMOLOGY .  melville\n",
       "2  ( Supplied by a Late Consumptive Usher to a Gr...  melville\n",
       "3  The pale Usher  threadbare in coat , heart , b...  melville\n",
       "4  \" While you take in hand to school others , an...  melville"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "moby_df = prepGutenTextParas('melville-moby_dick.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ Leaves of Grass by Walt Whitman 1855 ]</td>\n",
       "      <td>whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Come , said my soul , Such verses for my Body ...</td>\n",
       "      <td>whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Walt Whitman</td>\n",
       "      <td>whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ BOOK I . INSCRIPTIONS ]</td>\n",
       "      <td>whitman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>} One ' s - Self I Sing</td>\n",
       "      <td>whitman</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0           [ Leaves of Grass by Walt Whitman 1855 ]  whitman\n",
       "1  Come , said my soul , Such verses for my Body ...  whitman\n",
       "2                                       Walt Whitman  whitman\n",
       "3                          [ BOOK I . INSCRIPTIONS ]  whitman\n",
       "4                            } One ' s - Self I Sing  whitman"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "leaves_df = prepGutenTextParas('whitman-leaves.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tf-idf tool\n",
    "tfIdfV = TfidfVectorizer(ngram_range=(1, 2), \n",
    "                           max_features=5000, \n",
    "                           analyzer='word', \n",
    "                           lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfIdfFeats = tfIdfV.fit_transform(moby_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "cv = CountVectorizer(ngram_range=(1,2), max_features=2000, analyzer='word', lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mobyBOW = cv.fit_transform(moby_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book = gutenberg.raw('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern = \"[\\[].*?[\\]]\"\n",
    "book_noTitle = re.sub(pattern, \"\", book[0:999999])\n",
    "# recognize: the double dash '--'\n",
    "book_noTitle = re.sub(r'--',' ',book_noTitle)\n",
    "book_noTitle = re.sub(\"[\\[].*?[\\]]\", \"\", book_noTitle)\n",
    "book_noTitle = ' '.join(book_noTitle.split()) \n",
    "# Now we'll match and remove chapter headings.\n",
    "book_noChaps = re.sub(r'CHAPTER *.', '', book_noTitle)\n",
    "#book_noChaps = re.sub(r'Chapter \\d+', '', book_noChaps)\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "book_doc = nlp(book_noChaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moby = gutenberg.raw('melville-moby_dick.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Moby Dick by Herman Melville 1851]\r\n",
      "\r\n",
      "\r\n",
      "ETYMOLOGY.\r\n",
      "\r\n",
      "(Supplied by a Late Consumptive Usher to a Grammar School)\r\n",
      "\r\n",
      "The pale Usher--threadbare in coat, heart, body, and brain; I see him\r\n",
      "now.  He was\n"
     ]
    }
   ],
   "source": [
    "print (moby[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "ETYMOLOGY.\r\n",
      "\r\n",
      "(Supplied by a Late Consumptive Usher to a Grammar School)\r\n",
      "\r\n",
      "The pale Usher--threadbare in coat, heart, body, and brain; I see him\r\n",
      "now.  He was ever dusting his old lexicons and \n"
     ]
    }
   ],
   "source": [
    "# This pattern matches all text between square brackets.\n",
    "pattern = \"[\\[].*?[\\]]\"\n",
    "moby_noTitle = re.sub(pattern, \"\", moby)\n",
    "\n",
    "print(moby_noTitle[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we'll match and remove chapter headings.\n",
    "moby_noChaps = re.sub(r'CHAPTER .*', '', moby_noTitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETYMOLOGY. (Supplied by a Late Consumptive Usher to a Grammar School) The pale Usher--threadbare in \n"
     ]
    }
   ],
   "source": [
    "# Remove newlines and other extra whitespace by splitting and rejoining.\n",
    "moby_noWhite = ' '.join(moby_noChaps.split())\n",
    "\n",
    "print(moby_noWhite[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# All the processing work is done here, so it may take a while.\n",
    "moby_doc = nlp(moby_noWhite[0:999999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The moby_doc object is a <class 'spacy.tokens.doc.Doc'> object.\n",
      "It is 212262 tokens long\n",
      "The first three tokens are 'ETYMOLOGY. ('\n",
      "The type of each token is <class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# Let's explore the objects we've built.\n",
    "print(\"The moby_doc object is a {} object.\".format(type(moby_doc)))\n",
    "print(\"It is {} tokens long\".format(len(moby_doc)))\n",
    "print(\"The first three tokens are '{}'\".format(moby_doc[:3]))\n",
    "print(\"The type of each token is {}\".format(type(moby_doc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alice = gutenberg.raw('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once\n"
     ]
    }
   ],
   "source": [
    "print (alice[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\n",
      "book her sister was re\n"
     ]
    }
   ],
   "source": [
    "# This pattern matches all text between square brackets.\n",
    "pattern = \"[\\[].*?[\\]]\"\n",
    "alice_noTitle = re.sub(pattern, \"\", alice)\n",
    "\n",
    "print(alice_noTitle[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we'll match and remove chapter headings.\n",
    "alice_noChaps = re.sub(r'CHAPTER .*', '', alice_noTitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Utility function to calculate how frequently words appear in the text.\n",
    "def word_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of words.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            words.append(token.text)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 1694), (\"'s\", 1283), ('whale', 804), ('But', 584), ('like', 503), ('The', 470), ('ship', 407), ('man', 384), ('sea', 330), ('old', 319)]\n"
     ]
    }
   ],
   "source": [
    "moby_freq = word_frequencies(moby_doc, include_stop=False).most_common(10)\n",
    "print(moby_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to calculate how frequently lemas appear in the text.\n",
    "def lemma_frequencies(text, include_stop=True):\n",
    "    \n",
    "    # Build a list of lemas.\n",
    "    # Strip out punctuation and, optionally, stop words.\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moby: [('-PRON-', 2477), ('whale', 1331), (\"'s\", 968), ('but', 584), ('man', 576), ('the', 541), ('like', 527), ('ship', 503), ('be', 466), ('sea', 416)]\n"
     ]
    }
   ],
   "source": [
    "moby_lemma_freq = lemma_frequencies(moby_doc, include_stop=False).most_common(10)\n",
    "print('moby:', moby_lemma_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/christophersmyth/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "#moby_lemma = [ lmtzr.lemmatize(token) for token in moby_doc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mobySents = pd.DataFrame([[sent, \"melville\"] for sent in moby_doc.sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(ETYMOLOGY, .)</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((, Supplied, by, a, Late, Consumptive)</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Usher, to, a, Grammar, School, ))</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(The, pale, Usher, --, threadbare, in, coat, ,...</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(He, was, ever, dusting, his, old, lexicons, a...</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0         1\n",
       "0                                     (ETYMOLOGY, .)  melville\n",
       "1            ((, Supplied, by, a, Late, Consumptive)  melville\n",
       "2                 (Usher, to, a, Grammar, School, ))  melville\n",
       "3  (The, pale, Usher, --, threadbare, in, coat, ,...  melville\n",
       "4  (He, was, ever, dusting, his, old, lexicons, a...  melville"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobySents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_words = bag_of_words(moby_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n",
      "Processing row 5000\n",
      "Processing row 5500\n",
      "Processing row 6000\n",
      "Processing row 6500\n",
      "Processing row 7000\n",
      "Processing row 7500\n"
     ]
    }
   ],
   "source": [
    "moby_bow = bow_features(mobySents, common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-PRON-</th>\n",
       "      <th>whale</th>\n",
       "      <th>'s</th>\n",
       "      <th>but</th>\n",
       "      <th>man</th>\n",
       "      <th>the</th>\n",
       "      <th>like</th>\n",
       "      <th>ship</th>\n",
       "      <th>be</th>\n",
       "      <th>sea</th>\n",
       "      <th>...</th>\n",
       "      <th>historical</th>\n",
       "      <th>severe</th>\n",
       "      <th>solomon</th>\n",
       "      <th>firmly</th>\n",
       "      <th>stamp</th>\n",
       "      <th>publish</th>\n",
       "      <th>hemp</th>\n",
       "      <th>gam</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(ETYMOLOGY, .)</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>((, Supplied, by, a, Late, Consumptive)</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Usher, to, a, Grammar, School, ))</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(The, pale, Usher, --, threadbare, in, coat, ,...</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(He, was, ever, dusting, his, old, lexicons, a...</td>\n",
       "      <td>melville</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  -PRON- whale 's but man the like ship be sea     ...     historical severe  \\\n",
       "0      0     0  0   0   0   0    0    0  0   0     ...              0      0   \n",
       "1      0     0  0   0   0   0    0    0  0   0     ...              0      0   \n",
       "2      0     0  0   0   0   0    0    0  0   0     ...              0      0   \n",
       "3      1     0  0   0   0   1    0    0  0   0     ...              0      0   \n",
       "4      1     0  0   0   0   0    0    0  0   0     ...              0      0   \n",
       "\n",
       "  solomon firmly stamp publish hemp gam  \\\n",
       "0       0      0     0       0    0   0   \n",
       "1       0      0     0       0    0   0   \n",
       "2       0      0     0       0    0   0   \n",
       "3       0      0     0       0    0   0   \n",
       "4       0      0     0       0    0   0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0                                     (ETYMOLOGY, .)    melville  \n",
       "1            ((, Supplied, by, a, Late, Consumptive)    melville  \n",
       "2                 (Usher, to, a, Grammar, School, ))    melville  \n",
       "3  (The, pale, Usher, --, threadbare, in, coat, ,...    melville  \n",
       "4  (He, was, ever, dusting, his, old, lexicons, a...    melville  \n",
       "\n",
       "[5 rows x 2002 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moby_bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = []\n",
    "y = np.full(moby_bow.row.count(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moby_drop = pd.DataFrame()\n",
    "moby_drop = moby_bow.drop('text_source', axis=1)\n",
    "moby_drop = moby_drop.drop('text_sentence', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophersmyth/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/Users/christophersmyth/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/Users/christophersmyth/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/Users/christophersmyth/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/Users/christophersmyth/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/Users/christophersmyth/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/Users/christophersmyth/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/Users/christophersmyth/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/Users/christophersmyth/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n",
      "/Users/christophersmyth/anaconda3/lib/python3.6/site-packages/sklearn/naive_bayes.py:461: RuntimeWarning: divide by zero encountered in log\n",
      "  self.class_log_prior_ = (np.log(self.class_count_) -\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = BernoulliNB()\n",
    "cross_val_score(nb, moby_drop, y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alice_doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-03bc3acea08b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set up the bags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0malicewords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbag_of_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malice_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpersuasionwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbag_of_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersuasion_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Combine bags to create a set of unique words.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'alice_doc' is not defined"
     ]
    }
   ],
   "source": [
    "# Set up the bags.\n",
    "alicewords = bag_of_words(alice_doc)\n",
    "persuasionwords = bag_of_words(persuasion_doc)\n",
    "\n",
    "# Combine bags to create a set of unique words.\n",
    "common_words = set(alicewords + persuasionwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mobySents = [[sent, \"melville\"] for sent in moby_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "cv = CountVectorizer(ngram_range=(1,2), max_features=2000, analyzer='word', lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mobyBOW = cv.fit_transform(mobySents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_mobySents = pd.DataFrame(mobySents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_mobySents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_idfV = TfidfVectorizer(ngram_range=(1,2), \n",
    "                          max_features=5000, \n",
    "                          analyzer='word', \n",
    "                          lowercase=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_idfFeats = tf_idfV.fit_transform(mobySents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
